\documentclass{standalone}
% Preamble
\begin{document}


\subsection{Formules de Barnett et structure de l'algèbre quotient.}
Dans l'hypothèse où l'idéal est zéro-dimensionel, la dimension de l'algèbre quotient $A = \C[\bold{x}]/\langle f\rangle$ est finie, ce qui assure l'existence d'une base et de matrices compagnon $X_1,\cdots, X_n$ (matrices des opérateurs de multiplication par les variables dans la base considérée). Nous allons montrer que le même procédé mis en oeuvre dans la section \ref{Bar_gen}, consistant en manipulations sur les colonnes des matrices de Bezout $B(1), B(x_1), \cdots, B(x_n)$, permet ici aussi de construire une base de $A$ ainsi que les matrices compagnons associées. Rappelons tout d'abord un certain nombre de propriétés algébriques du polynôme $\delta(1)$ et des matrices de Bezout $B(x_k)$.

\subsubsection{Propriétés algébriques du polynôme $\delta(1)$ et de la matrice $B(1)$}
Les propriétés qui suivent sont de nature algébrique et sont données sans démonstration. Le lecteur intéressé pourra consulter les détails dans \cite{jpc}. Comme dans la proposition \ref{Barnett}, définissons de nouvelles familles d'éléments de $A$ par les produits vecteur-matrice :
\begin{equation}
		\hat{\bold{x}}_k  =  \bold{x}B(x_k), \quad k=0\cdots n
\end{equation}
avec la convention de notation habituelle $\hat{\bold{x}}_0 = \hat{\bold{x}}$.
\begin{exmp}
En reprenant l'exemple \ref{bez_multi} nous avons
\begin{equation}
	\begin{array}{lll}
		\hat{\bold{x}}_0 & = & (0, -x_2 - x_1x_2^2, -1 - x_1x_2, x_1, -x_2, 1) \\
		\hat{\bold{x}}_1 & = & (0, 0, 0, -1 - x_2^2, -x_1x_2, x_1) \\
		\hat{\bold{x}}_2 & = & (-1 - x_1x_2, -x_2 - x_2^2 - x_1, - x_2 - x_1x_2^2, x_1x_2, -x_2^2, x_2)
	\end{array}
\end{equation}
\end{exmp}

\begin{prop}
\label{xj} (admise, démonstration dans \cite{jpc}).
Pour tout $k=1\cdots n$ on a
\begin{equation}
    \hat{\bold{x}}_0x_k = \hat{\bold{x}}_k
\end{equation}
\end{prop}
Les relations ci-dessus sont faciles à vérifier sur l'exemple \ref{bez_multi}. Jusqu'à maintenant les cas univariable et multivariable sont très similaires, sauf sur un point: dans le cas multivariable les familles $\bold{x}$ et $\hat{\bold{x}}$ ne sont plus nécessairement des bases de $A$. On a cependant la propriété suivante

\begin{prop} (admise, démonstration dans \cite{jpc}).
Chacune des familles $\bold{x}$ et $\hat{\bold{x}}$ est génératrice dans $A$.
\end{prop}

\subsubsection{Processus de réduction}
\label{sec:reduction_process}
La proposition précédente fournit un début de structure de l'algèbre $A$. Nous avons pour l'instant une famille génératrice $\bold{x}$ de $A$ ainsi que des matrices de Bezout $B(x_k), k = 0, \cdots, n$. Nous allons montrer comment, en appliquant le procédé matriciel décrit dans la section \ref{Bar_gen} à la famille génératrice $\bold{x}$ et aux matrices de Bezout $B(x_k)$, on peut fabriquer une base de $A$ et des matrices compagnon $X_k$. Illustrons les calculs à partir de l'exemple \ref{bez_multi}. Le rang de $B(1)$ est $5$. La première colonne de $B(x_1)$ est nulle mais celle de $B(x_2)$ ne l'est pas, ce qui fournit la relation dans le quotient $1 + x_1x_2 = 0$.
Multiplions $\bold{x}$ à droite par la matrice de Gauss $P$ dont la cinquième colonne vaut $(1, 0, 0, 0, 1, 0)^{T}$, et multiplions les matrices de Bezout à gauche par $P^{-1}$, ce qui revient à soustraire la cinquième ligne à la première.
Les matrices de Bezout $B(1), B(x_1), B(x_2)$ s'écrivent:
$$
\begin{array}{c|cccccc}
	B(1) & 1 & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1 &  &  &  &  &  & 1\\
	x_2 &  & -1 &  &  & -1 & \\
	x_2^2 &  &  &  &  &  & \\
	x_1 &  &  &  & 1 &  & \\
	1+x_1x_2 &  &  & -1 &  &  & \\
	x_1x_2^2 &  & -1 &  &  &  &
\end{array}$$
$$
\begin{array}{c|cccccc}
	B(x_1) & 1 & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1 &  &  &  & 1 & 1 & \\
	x_2 &  &  &  &  &  & \\
	x_2^2 &  &  &  &  &  & \\
	x_1 &  &  &  &  &  & 1\\
	1+x_1x_2 &  &  &  &  & -1 & \\
	x_1x_2^2 &  &  &  & -1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|cccccc}
	B(x_2) & 1 & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1 &  &  &  & -1 &  & \\
	x_2 &  & -1 & -1 &  &  & 1\\
	x_2^2 &  & -1 &  &  & -1 & \\
	x_1 &  & -1 &  &  &  & \\
	1+x_1x_2 & -1 &  &  & 1 &  & \\
	x_1x_2^2 &  &  & -1 &  &  &
\end{array}
$$
La première colonne de $B(x_2)$ contient maintenant un seul coefficient non nul, indexé par $1 + x_1x_2$. On peut donc, en projetant les trois bezoutiens sur $A_x$, supprimer la première colonne et la cinquième ligne dans les trois matrices:
$$
\begin{array}{c|ccccc}
	B(1) & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  &  &  & 1 \\
	x_2  & -1 &  &  & -1 & \\
	x_2^2  &  &  &  &  & \\
	x_1  &  &  & 1 &  & \\
	x_1x_2^2  & -1 &  &  &  &
\end{array}$$
$$
\begin{array}{c|ccccc}
	B(x_1)  & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & 1 & 1 & \\
	x_2  &  &  &  &  & \\
	x_2^2  &  &  &  &  & \\
	x_1  &  &  &  &  & 1 \\
	x_1x_2^2  &  &  & -1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|ccccc}
	B(x_2) & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & -1 &  & \\
	x_2  & -1 & -1 &  &  & 1 \\
	x_2^2  & -1 &  &  & -1 & \\
	x_1  & -1 &  &  &  & \\
	x_1x_2^2 &  & -1 &  &  &
\end{array}
$$

La deuxième colonne de $B(1)$ est nulle, celle de $B(x_2)$ ne l'est pas. La relation est $x_2 + x_1x_2^{2} = 0$. La matrice $P$ est définie par sa cinquième colonne $(0, 1, 0, 0, 1)^{T}$. Le vecteur $\bold{x}$ devient $(1, x_2, x_2^{2}, x_1, x_2 + x_1x_2^{2})$. On soustrait la cinquième ligne à la deuxième. Les bezoutiens s'écrivent:
$$
\begin{array}{c|ccccc}
	B(1) & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  &  &  & 1 \\
	x_2  &  &  &  & -1 & \\
	x_2^2  &  &  &  &  & \\
	x_1  &  &  & 1 &  & \\
	x_2 + x_1x_2^2  & -1 &  &  &  &
\end{array}$$
$$
\begin{array}{c|ccccc}
	B(x_1)  & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & 1 & 1 & \\
	x_2  &  &  & 1 &  & \\
	x_2^2  &  &  &  &  & \\
	x_1  &  &  &  &  & 1 \\
	x_2 + x_1x_2^2  &  &  & -1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|ccccc}
	B(x_2) & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & -1 &  & \\
	x_2  & -1 &  &  &  & 1 \\
	x_2^2  & -1 &  &  & -1 & \\
	x_1  & -1 &  &  &  & \\
	x_2 + x_1x_2^2 &  & -1 &  &  &
\end{array}
$$

La deuxième colonne de $B_2$ contient un seul coefficient non nul, en cinquième ligne, on peut donc supprimer les deuxièmes colonnes et les cinquièmes lignes:

$$
\begin{array}{c|cccc}
	B(1) & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &   &  &  & 1 \\
	x_2  &  &  & -1 & \\
	x_2^2  &  &  &  & \\
	x_1  &  & 1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|cccc}
	B(x_1)  & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  & 1 & 1 & \\
	x_2  &  & 1 &  & \\
	x_2^2  &  &  &  & \\
	x_1  &  &  &  & 1
\end{array}
\hspace{0.2cm}
\begin{array}{c|cccc}
	B(x_2) & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  & -1 &  & \\
	x_2  & -1 &  &  & 1 \\
	x_2^2  & -1 &  & -1 & \\
	x_1  & -1 &  &  &
\end{array}
$$
La première colonne de $B(1)$ est nulle, celle de $B(x_2)$ ne l'est pas. La relation est $x_2 + x_2^{2} + x_1 = 0$. La matrice $P$ est définie par sa quatrième colonne $(0, 1, 1, 1)^{T}$. Le vecteur $\bold{x}$ devient $(1, x_2, x_2^{2},  x_2 + x_2^{2} + x_1)$. On soustrait la quatrième ligne à la deuxième et à la troisième. Les bezoutiens s'écrivent:

$$
\begin{array}{c|cccc}
	B(1) & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &   &  &  & 1 \\
	x_2  &  & -1 & -1 & \\
	x_2^2  &  & -1 &  & \\
	x_2 + x_2^{2} + x_1  &  & 1 &  &
\end{array}$$
$$
\begin{array}{c|cccc}
	B(x_1)  & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  & 1 & 1 & \\
	x_2  &  &  &  & \\
	x_2^2  &  &  &  & -1 \\
	x_2 + x_2^{2} + x_1  &  &  &  & 1
\end{array}
\hspace{0.2cm}
\begin{array}{c|cccc}
	B(x_2) & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  & -1 &  & \\
	x_2  &  &  &  & 1 \\
	x_2^2  &  &  & -1 & \\
	x_2 + x_2^{2} + x_1  & -1 &  &  &
\end{array}$$

La première colonne de $B_2$ contient un seul coefficient non nul, en quatrième ligne, on peut donc supprimer les premières colonnes et les quatrièmes lignes:
$$
\begin{array}{c|ccc}
	B(1) & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & 1 \\
	x_2  & -1 & -1 & \\
	x_2^2 & -1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|ccc}
	B(x_1) & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  & 1 & 1 & \\
	x_2  & 1 &  & -1\\
	x_2^2  &  &  & -1
\end{array}
\hspace{0.2cm}
\begin{array}{c|ccc}
	B(x_2) & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  & -1 &  & \\
	x_2  &  &  & 1 \\
	x_2^2  &  & -1 &
\end{array}$$
A ce stade, la matrice $B(1)$ est inversible et le processus de réduction est donc terminé. On vérifie que les familles $\bold{x} = (1, x_2, x_2^{2})$ et $\bold{y} = (y_1, y_1^{2}, y_1^{3})$ sont des bases de $A$, dont les bases de Horner associées sont $\hat{\bold{x}} = (-x_2-x_2^{2}, -x_{2}, 1)$ et
$\hat{\bold{y}} = (y_1^{3}, -y_1^{2}-y_1^{2}y_2, -y_1^{2})$. La dimension de $A$ est ici égale à~$3$.
D'une façon générale nous avons (\cite{jpc} p.57, \cite{bm}, \cite{tm})
\begin{prop}
	\label{conjecture}
Lorsque le processus de réduction est terminé, c'est-à-dire lorsque la matrice $B(1)$ est inversible et que toutes les matrices $B(x_k), k=0, \cdots, n$ sont de même taille et indexées par des familles de polynômes $\bold{x, y}$, alors chacune des familles $\bold{x, y}$ est une base de $A$.
\end{prop}

\begin{rem}
Nous insistons sur le fait que la proposition précédente est valable uniquement dans l'hypothèse où l'idéal est zéro-dimensionnel. Lors de nos expériences nous avons pu observer que dans le cas contraire, il est possible d'obtenir à la fin du processus de réduction des matrices $B(1)$ de tailles différentes suivant que l'on utilise les relations en $x$ ou en $y$ lors du processus de réduction. Le cas échéant, cette différence de taille finale est un phénomène qui reste à éclaircir.
\end{rem}

\subsubsection{Formules de Barnett et matrices compagnon}
Reprenons l'exemple précédent et définissons les matrices $X_1, X_2$ par les quotients
\begin{equation}
	X_1 = B(x_1)B(1)^{-1} =
	\begin{bmatrix}
		0 & -1 & 0\\
		-1 & 0 & -1\\
		-1 & 0 & 0
	\end{bmatrix},\quad
	X_2 = B(x_2)B(1)^{-1} =
	\begin{bmatrix}
		0 & 0 & 1\\
		1 & 0 & 0\\
		0 & 1 & -1
	\end{bmatrix}
\end{equation}
On vérifie que $X_1, X_2$ sont les matrices de multiplication par les variables $x_1, x_2$ dans la base $\bold{x}$ et sont donc les matrices compagnon associées à la base $\bold{x}$. D'une façon générale nous avons:
\begin{prop}
\label{Barnett_multi}
Lorsque le processus de réduction est terminé et que les matrices de Bezout sont écrites dans des bases $\bold{x, y}$, alors les matrices compagnon $X_j$, c'est à dire les matrices de multiplication par $x_j$ dans la base $\bold{x}$, peuvent se calculer grâce aux {\bf formules de Barnett}
\begin{equation}
	X_j = B(x_j)B(1)^{-1}
\end{equation}
\end{prop}

\begin{rem}
Comme dans le cas univariable nous avons pour tout $j=1,\cdots,n$,\\
$B(x_j)^{T}B(1)^{-T}$ est la matrice de multiplication par $y_j$ dans la base $\bold{y}$ \\
$B(1)^{-1}B(x_j)$ est la matrice de multiplication par $x_j$ dans la base $\hat{\bold{x}}$ \\
$B(1)^{-T}B(x_j)^{T}$ est la matrice de multiplication par $y_j$ dans la base $\hat{\bold{y}}$
\end{rem}

\subsubsection{Calcul numérique des racines}
Comme dans le cas univariable (voir Proposition \ref{compan2roots}) les racines du système polynomial $f_1, \cdots, f_n$ s'obtiennent numériquement en calculant les valeurs propres des matrices compagnons (\cite{AS}). Dans cet exemple les matrices $X_1, X_2$ fournissent les valeurs propres
$$
\begin{array}{c|c}
	x_1 & x_2 \\
	\hline
	-1.32472  & 0.75488 \\
	0.66236 + 0.56228i & -0.87744 + 0.74486i \\
	0.66236 - 0.56228i & -0.87744 - 0.74486i
\end{array}
$$
Puisque l'algèbre $A$ est commutative, les matrices $X_1, X_2$ commutent et ont donc les mêmes vecteurs propres. Lors du calcul il faut donc faire attention d'ordonner les valeurs propres pour qu'elles correspondent aux mêmes vecteurs propres. Dans l'exemple précédent on vérifie facilement que les couples $(x_1, x_2)$ ci-dessus sont bien des approximations des racines du système $f_1 = x_1^2 + x_1x_2^2 - 1, f_2 = x_1^2x_2 + x_1$.


\end{document}
